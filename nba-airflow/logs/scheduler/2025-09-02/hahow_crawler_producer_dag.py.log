[2025-09-02T21:14:51.609+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=87) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:14:51.612+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:14:51.624+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:14:51.623+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:14:59.903+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:15:00.072+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:15:00.684+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:15:00.679+0800] INFO - override.py:1911 - create_permission() - Created Permission View: can edit on DAG:hahow_crawler_producer
[2025-09-02T21:15:00.743+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:15:00.740+0800] INFO - override.py:1911 - create_permission() - Created Permission View: can delete on DAG:hahow_crawler_producer
[2025-09-02T21:15:00.769+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:15:00.768+0800] INFO - override.py:1911 - create_permission() - Created Permission View: can read on DAG:hahow_crawler_producer
[2025-09-02T21:15:00.793+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:15:00.792+0800] INFO - override.py:1911 - create_permission() - Created Permission View: can delete on DAG Run:hahow_crawler_producer
[2025-09-02T21:15:00.820+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:15:00.819+0800] INFO - override.py:1911 - create_permission() - Created Permission View: can create on DAG Run:hahow_crawler_producer
[2025-09-02T21:15:00.864+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:15:00.863+0800] INFO - override.py:1911 - create_permission() - Created Permission View: menu access on DAG Run:hahow_crawler_producer
[2025-09-02T21:15:00.900+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:15:00.899+0800] INFO - override.py:1911 - create_permission() - Created Permission View: can read on DAG Run:hahow_crawler_producer
[2025-09-02T21:15:00.901+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:15:00.900+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:15:00.954+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:15:00.953+0800] INFO - dag.py:3262 - bulk_write_to_db() - Creating ORM DAG for hahow_crawler_producer
[2025-09-02T21:15:00.989+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:15:00.989+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-08-31 18:00:00+00:00, run_after=2025-09-01 18:00:00+00:00
[2025-09-02T21:15:01.090+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 9.518 seconds
[2025-09-02T21:15:31.595+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=139) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:15:31.601+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:15:31.605+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:15:31.604+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:15:33.160+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:15:33.193+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:15:33.248+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:15:33.248+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:15:33.300+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:15:33.299+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-08-31 18:00:00+00:00, run_after=2025-09-01 18:00:00+00:00
[2025-09-02T21:15:33.336+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.760 seconds
[2025-09-02T21:16:04.035+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=180) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:16:04.045+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:16:04.059+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:16:04.051+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:16:09.316+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:16:09.525+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:16:09.825+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:16:09.824+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:16:10.102+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 6.159 seconds
[2025-09-02T21:16:46.658+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=435) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:16:46.675+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:16:46.696+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:16:46.694+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:16:51.116+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:16:51.169+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:16:51.271+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:16:51.271+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:16:51.399+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 4.822 seconds
[2025-09-02T21:17:21.590+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=480) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:17:21.595+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:17:21.602+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:17:21.600+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:17:24.652+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:17:24.699+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:17:24.799+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:17:24.799+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:17:24.969+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 3.400 seconds
[2025-09-02T21:17:56.061+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=585) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:17:56.069+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:17:56.096+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:17:56.096+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:18:01.427+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:18:01.564+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:18:01.675+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:18:01.674+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:18:01.840+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 5.932 seconds
[2025-09-02T21:18:31.977+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=644) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:18:31.980+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:18:31.984+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:18:31.983+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:18:33.276+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:18:33.319+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:18:33.420+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:18:33.418+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:18:33.506+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:18:33.505+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:18:33.571+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.612 seconds
[2025-09-02T21:19:04.129+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=685) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:19:04.133+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:19:04.136+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:19:04.136+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:19:05.466+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:19:05.488+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:19:05.526+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:19:05.525+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:19:05.559+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:19:05.558+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:19:05.590+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.478 seconds
[2025-09-02T21:19:35.834+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=726) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:19:35.837+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:19:35.844+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:19:35.843+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:19:37.379+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:19:37.410+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:19:37.462+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:19:37.462+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:19:37.503+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:19:37.502+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:19:37.551+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.737 seconds
[2025-09-02T21:20:07.693+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=767) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:20:07.697+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:20:07.705+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:20:07.704+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:20:09.216+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:20:09.242+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:20:09.291+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:20:09.291+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:20:09.353+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:20:09.353+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:20:09.400+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.725 seconds
[2025-09-02T21:20:39.984+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=808) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:20:39.992+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:20:40.004+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:20:40.002+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:20:41.493+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:20:41.513+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:20:41.562+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:20:41.562+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:20:41.612+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:20:41.612+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:20:41.652+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.693 seconds
[2025-09-02T21:21:12.077+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=849) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:21:12.083+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:21:12.094+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:21:12.093+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:21:13.593+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:21:13.613+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:21:13.653+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:21:13.653+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:21:13.699+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:21:13.698+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:21:13.727+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.693 seconds
[2025-09-02T21:21:43.905+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=879) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:21:43.911+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:21:43.915+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:21:43.915+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:21:45.662+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:21:45.697+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:21:45.746+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:21:45.746+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:21:45.789+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:21:45.789+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:21:45.824+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.936 seconds
[2025-09-02T21:22:16.358+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=920) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:22:16.361+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:22:16.365+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:22:16.364+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:22:17.813+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:22:17.836+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:22:17.895+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:22:17.895+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:22:17.944+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:22:17.943+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:22:17.974+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.630 seconds
[2025-09-02T21:22:48.561+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=961) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:22:48.567+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:22:48.572+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:22:48.571+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:22:50.167+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:22:50.193+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:22:50.288+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:22:50.288+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:22:50.421+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:22:50.420+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:22:50.462+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.920 seconds
[2025-09-02T21:23:21.036+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=1002) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:23:21.038+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:23:21.042+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:23:21.041+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:23:23.279+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:23:23.306+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:23:23.379+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:23:23.378+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:23:23.448+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:23:23.447+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:23:23.500+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 2.479 seconds
[2025-09-02T21:23:53.798+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=1053) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:23:53.809+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:23:53.817+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:23:53.816+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:23:55.234+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:23:55.267+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:23:55.318+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:23:55.318+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:23:55.364+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:23:55.364+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:23:55.394+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.623 seconds
[2025-09-02T21:24:25.748+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=1084) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:24:25.750+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:24:25.758+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:24:25.757+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:24:27.509+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:24:27.556+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:24:27.636+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:24:27.635+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:24:27.695+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:24:27.694+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:24:27.740+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 2.009 seconds
[2025-09-02T21:24:58.375+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=1125) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:24:58.383+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:24:58.390+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:24:58.389+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:25:00.536+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:25:00.571+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:25:00.620+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:25:00.619+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:25:00.674+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:25:00.674+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:25:00.705+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 2.354 seconds
[2025-09-02T21:25:31.381+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=1166) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:25:31.384+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:25:31.391+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:25:31.390+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:25:32.988+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:25:33.026+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:25:33.084+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:25:33.084+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:25:33.141+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:25:33.141+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:25:33.181+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.825 seconds
[2025-09-02T21:26:03.956+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=1207) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:26:03.961+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:26:03.970+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:26:03.969+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:26:05.520+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:26:05.557+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:26:05.614+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:26:05.614+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:26:05.668+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:26:05.668+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:26:05.702+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.790 seconds
[2025-09-02T21:26:36.279+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=1248) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:26:36.281+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:26:36.284+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:26:36.284+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:26:38.210+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:26:38.251+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:26:38.339+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:26:38.338+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:26:38.424+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:26:38.424+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:26:38.475+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 2.211 seconds
[2025-09-02T21:27:08.634+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=1289) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:27:08.637+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:27:08.643+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:27:08.642+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:27:10.393+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:27:10.425+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:27:10.484+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:27:10.483+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:27:10.532+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:27:10.531+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:27:10.575+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.956 seconds
[2025-09-02T21:27:41.299+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=1330) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:27:41.302+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:27:41.317+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:27:41.315+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:27:43.230+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:27:43.281+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:27:43.405+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:27:43.404+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:27:43.498+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:27:43.498+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:27:43.560+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 2.294 seconds
[2025-09-02T21:28:13.817+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=1371) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:28:13.831+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:28:13.852+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:28:13.849+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:28:16.216+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:28:16.259+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:28:16.338+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:28:16.338+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:28:16.403+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:28:16.403+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:28:16.452+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 2.693 seconds
[2025-09-02T21:28:47.248+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=1412) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:28:47.251+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:28:47.255+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:28:47.255+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:28:49.242+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:28:49.305+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:28:49.389+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:28:49.388+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:28:49.442+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:28:49.441+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:28:49.475+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 2.249 seconds
[2025-09-02T21:29:20.190+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=1453) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:29:20.191+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:29:20.195+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:29:20.194+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:29:21.644+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:29:21.677+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:29:21.726+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:29:21.724+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:29:21.773+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:29:21.772+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:29:21.807+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.630 seconds
[2025-09-02T21:29:52.484+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=1494) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:29:52.490+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:29:52.494+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:29:52.494+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:29:54.138+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:29:54.196+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:29:54.282+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:29:54.282+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:29:54.356+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:29:54.355+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:29:54.400+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.928 seconds
[2025-09-02T21:30:25.043+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=1535) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:30:25.044+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:30:25.049+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:30:25.048+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:30:26.493+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:30:26.522+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:30:26.586+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:30:26.585+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:30:26.630+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:30:26.630+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:30:26.668+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.639 seconds
[2025-09-02T21:30:57.440+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=1576) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:30:57.442+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:30:57.446+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:30:57.445+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:30:58.877+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:30:58.934+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:30:59.062+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:30:59.061+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:30:59.165+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:30:59.164+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:30:59.241+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.813 seconds
[2025-09-02T21:31:29.933+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=1617) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:31:29.935+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:31:29.938+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:31:29.937+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:31:31.372+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:31:31.399+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:31:31.458+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:31:31.457+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:31:31.506+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:31:31.505+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:31:31.557+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.636 seconds
[2025-09-02T21:32:02.238+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=1658) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:32:02.243+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:32:02.246+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:32:02.246+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:32:03.631+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:32:03.662+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:32:03.729+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:32:03.728+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:32:03.798+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:32:03.798+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:32:03.844+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.619 seconds
[2025-09-02T21:32:34.985+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=1710) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:32:34.989+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:32:34.994+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:32:34.993+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:32:37.994+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:32:38.051+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:32:38.168+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:32:38.165+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:32:38.250+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:32:38.246+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:32:38.332+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 3.380 seconds
[2025-09-02T21:33:08.887+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=1756) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:33:08.895+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:33:08.903+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:33:08.901+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:33:11.108+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:33:11.133+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:33:11.186+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:33:11.185+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:33:11.222+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:33:11.221+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:33:11.257+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 2.404 seconds
[2025-09-02T21:33:41.513+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=1807) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:33:41.516+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:33:41.522+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:33:41.521+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:33:42.902+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:33:42.928+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:33:42.965+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:33:42.965+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:33:43.004+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:33:43.003+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:33:43.041+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.550 seconds
[2025-09-02T21:34:13.651+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=1848) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:34:13.658+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:34:13.662+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:34:13.662+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:34:14.985+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:34:15.009+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:34:15.054+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:34:15.054+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:34:15.106+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:34:15.106+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:34:15.145+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.514 seconds
[2025-09-02T21:34:45.728+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=1889) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:34:45.731+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:34:45.735+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:34:45.734+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:34:46.976+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:34:46.998+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:34:47.044+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:34:47.044+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:34:47.081+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:34:47.080+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:34:47.127+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.415 seconds
[2025-09-02T21:35:17.263+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=1930) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:35:17.266+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:35:17.273+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:35:17.272+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:35:18.557+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:35:18.589+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:35:18.646+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:35:18.646+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:35:18.698+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:35:18.698+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:35:18.745+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.503 seconds
[2025-09-02T21:35:48.845+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=1971) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:35:48.851+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:35:48.856+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:35:48.855+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:35:50.317+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:35:50.343+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:35:50.386+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:35:50.385+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:35:50.429+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:35:50.429+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:35:50.464+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.635 seconds
[2025-09-02T21:36:20.764+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=2012) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:36:20.767+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:36:20.776+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:36:20.774+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:36:22.168+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:36:22.192+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:36:22.250+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:36:22.245+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:36:22.298+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:36:22.298+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:36:22.330+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.599 seconds
[2025-09-02T21:36:52.976+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=2053) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:36:52.982+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:36:52.986+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:36:52.985+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:36:54.274+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:36:54.305+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:36:54.379+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:36:54.378+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:36:54.492+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:36:54.492+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:36:54.533+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.572 seconds
[2025-09-02T21:37:25.125+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=2094) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:37:25.128+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:37:25.134+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:37:25.134+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:37:26.310+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:37:26.330+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:37:26.369+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:37:26.368+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:37:26.403+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:37:26.403+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:37:26.430+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.326 seconds
[2025-09-02T21:37:56.855+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=2135) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:37:56.858+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:37:56.863+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:37:56.863+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:37:58.189+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:37:58.223+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:37:58.267+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:37:58.267+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:37:58.309+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:37:58.308+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:37:58.344+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.509 seconds
[2025-09-02T21:38:28.916+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=2176) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:38:28.918+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:38:28.925+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:38:28.925+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:38:30.365+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:38:30.383+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:38:30.419+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:38:30.419+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:38:30.454+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:38:30.453+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:38:30.488+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.595 seconds
[2025-09-02T21:39:01.059+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=2217) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:39:01.062+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:39:01.067+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:39:01.066+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:39:02.309+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:39:02.332+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:39:02.380+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:39:02.380+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:39:02.416+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:39:02.416+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:39:02.452+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.410 seconds
[2025-09-02T21:39:32.929+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=2258) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:39:32.932+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:39:32.940+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:39:32.939+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:39:34.259+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:39:34.283+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:39:34.333+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:39:34.332+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:39:34.390+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:39:34.389+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:39:34.435+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.531 seconds
[2025-09-02T21:40:05.123+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=2299) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:40:05.131+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:40:05.138+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:40:05.137+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:40:06.568+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:40:06.588+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:40:06.628+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:40:06.628+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:40:06.668+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:40:06.668+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:40:06.702+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.607 seconds
[2025-09-02T21:40:37.251+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=2340) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:40:37.256+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:40:37.265+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:40:37.264+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:40:38.596+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:40:38.616+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:40:38.654+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:40:38.653+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:40:38.692+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:40:38.692+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:40:38.725+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.497 seconds
[2025-09-02T21:41:09.277+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=2381) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:41:09.279+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:41:09.286+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:41:09.285+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:41:10.764+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:41:10.781+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:41:10.822+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:41:10.822+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:41:10.855+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:41:10.855+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:41:10.884+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.632 seconds
[2025-09-02T21:41:41.361+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=2422) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:41:41.366+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:41:41.375+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:41:41.374+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:41:42.825+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:41:42.858+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:41:42.908+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:41:42.907+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:41:42.948+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:41:42.948+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:41:42.988+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.655 seconds
[2025-09-02T21:42:13.665+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=2463) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:42:13.667+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:42:13.674+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:42:13.673+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:42:15.088+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:42:15.117+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:42:15.162+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:42:15.161+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:42:15.204+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:42:15.203+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:42:15.235+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.585 seconds
[2025-09-02T21:42:45.663+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=2504) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:42:45.669+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:42:45.673+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:42:45.672+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:42:48.091+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:42:48.160+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:42:48.308+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:42:48.307+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:42:48.440+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:42:48.438+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:42:48.526+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 2.884 seconds
[2025-09-02T21:43:19.001+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=2547) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:43:19.004+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:43:19.011+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:43:19.010+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:43:21.797+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:43:21.850+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:43:22.036+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:43:22.035+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:43:22.155+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:43:22.155+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:43:22.210+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 3.229 seconds
[2025-09-02T21:43:52.909+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=2588) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:43:52.914+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:43:52.918+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:43:52.917+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:43:55.537+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:43:55.583+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:43:55.684+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:43:55.683+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:43:55.780+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:43:55.779+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:43:55.867+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 2.969 seconds
[2025-09-02T21:44:26.120+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=2634) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:44:26.121+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:44:26.125+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:44:26.124+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:44:27.458+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:44:27.502+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:44:27.572+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:44:27.572+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:44:27.626+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:44:27.625+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:44:27.667+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.558 seconds
[2025-09-02T21:44:57.846+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=2678) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:44:57.859+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:44:57.875+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:44:57.874+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:45:01.484+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:45:01.540+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:45:01.630+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:45:01.630+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:45:01.720+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:45:01.719+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:45:01.796+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 4.005 seconds
[2025-09-02T21:45:32.591+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=2746) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:45:32.594+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:45:32.598+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:45:32.597+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:45:33.728+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:45:33.748+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:45:33.789+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:45:33.788+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:45:33.829+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:45:33.828+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:45:33.861+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.285 seconds
[2025-09-02T21:46:04.379+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=2797) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:46:04.381+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:46:04.385+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:46:04.384+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:46:05.670+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:46:05.695+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:46:05.755+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:46:05.754+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:46:05.813+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:46:05.813+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:46:05.860+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.492 seconds
[2025-09-02T21:46:36.390+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=2858) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:46:36.396+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:46:36.400+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:46:36.400+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:46:37.963+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:46:37.993+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:46:38.049+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:46:38.048+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:46:38.112+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:46:38.112+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:46:38.155+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.779 seconds
[2025-09-02T21:47:08.287+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=2939) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:47:08.289+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:47:08.293+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:47:08.292+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:47:09.289+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:47:09.309+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:47:09.345+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:47:09.345+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:47:09.378+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:47:09.377+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:47:09.406+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.130 seconds
[2025-09-02T21:47:39.748+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=2985) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:47:39.761+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:47:39.777+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:47:39.776+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:47:41.863+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:47:41.887+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:47:41.937+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:47:41.937+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:47:41.977+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:47:41.977+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:47:42.023+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 2.309 seconds
[2025-09-02T21:48:12.263+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=3056) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:48:12.265+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:48:12.269+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:48:12.268+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:48:13.501+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:48:13.531+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:48:13.579+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:48:13.579+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:48:13.622+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:48:13.621+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:48:13.664+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.412 seconds
[2025-09-02T21:48:44.067+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=3125) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:48:44.070+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:48:44.077+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:48:44.076+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:48:47.223+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:48:47.267+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:48:47.330+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:48:47.330+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:48:47.391+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:48:47.390+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:48:47.447+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 3.403 seconds
[2025-09-02T21:49:17.755+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=3171) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:49:17.763+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:49:17.771+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:49:17.770+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:49:19.387+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:49:19.412+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:49:19.471+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:49:19.470+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:49:19.521+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:49:19.521+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:49:19.572+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.851 seconds
[2025-09-02T21:49:50.193+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=3217) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:49:50.196+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:49:50.206+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:49:50.205+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:49:51.766+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:49:51.808+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:49:51.900+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:49:51.900+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:49:51.971+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:49:51.970+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:49:52.030+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.872 seconds
[2025-09-02T21:50:22.620+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=3263) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:50:22.626+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:50:22.631+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:50:22.630+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:50:24.058+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:50:24.078+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:50:24.119+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:50:24.119+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:50:24.154+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:50:24.154+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:50:24.188+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.587 seconds
[2025-09-02T21:50:54.757+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=3309) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:50:54.760+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:50:54.767+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:50:54.766+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:50:56.151+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:50:56.178+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:50:56.218+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:50:56.217+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:50:56.261+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:50:56.260+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:50:56.301+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.564 seconds
[2025-09-02T21:51:26.999+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=3355) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:51:27.002+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:51:27.009+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:51:27.008+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:51:28.368+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:51:28.396+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:51:28.458+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:51:28.457+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:51:28.517+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:51:28.517+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:51:28.566+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.584 seconds
[2025-09-02T21:51:59.466+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=3403) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:51:59.467+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:51:59.471+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:51:59.471+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:52:01.166+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:52:01.185+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:52:01.225+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:52:01.224+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:52:01.261+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:52:01.261+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:52:01.298+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.845 seconds
[2025-09-02T21:52:32.144+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=3447) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:52:32.153+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:52:32.166+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:52:32.164+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:52:34.595+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:52:34.615+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:52:34.659+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:52:34.659+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:52:34.694+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:52:34.693+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:52:34.727+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 2.677 seconds
[2025-09-02T21:53:05.425+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=3498) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:53:05.437+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:53:05.452+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:53:05.451+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:53:08.497+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:53:08.576+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:53:08.712+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:53:08.712+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:53:08.847+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:53:08.847+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:53:08.930+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 3.552 seconds
[2025-09-02T21:53:39.096+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=3549) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:53:39.099+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:53:39.106+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:53:39.105+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:53:42.130+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:53:42.157+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:53:42.234+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:53:42.233+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:53:42.420+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:53:42.419+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:53:42.498+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 3.428 seconds
[2025-09-02T21:54:12.709+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=3605) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:54:12.726+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:54:12.739+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:54:12.738+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:54:15.622+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:54:15.661+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:54:15.755+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:54:15.755+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:54:15.851+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:54:15.850+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:54:15.915+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 3.278 seconds
[2025-09-02T21:54:46.031+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=3653) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:54:46.035+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:54:46.040+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:54:46.039+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:54:47.393+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:54:47.414+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:54:47.457+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:54:47.456+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:54:47.496+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:54:47.496+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:54:47.543+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.526 seconds
[2025-09-02T21:55:17.949+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=3731) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:55:17.952+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:55:17.975+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:55:17.974+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:55:20.785+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:55:20.875+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:55:21.005+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:55:21.005+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:55:21.190+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 3.270 seconds
[2025-09-02T21:55:51.283+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=3823) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:55:51.286+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:55:51.291+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:55:51.290+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:55:52.528+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:55:52.547+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:55:52.585+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:55:52.585+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:55:52.620+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:55:52.619+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:55:52.649+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.377 seconds
[2025-09-02T21:56:23.534+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=3869) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:56:23.536+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:56:23.538+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:56:23.538+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:56:24.829+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:56:24.852+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:56:24.895+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:56:24.895+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:56:24.936+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:56:24.935+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:56:24.964+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.441 seconds
[2025-09-02T21:56:55.432+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=3915) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:56:55.434+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:56:55.439+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:56:55.438+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:56:56.621+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:56:56.648+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:56:56.695+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:56:56.694+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:56:56.740+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:56:56.740+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:56:56.771+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.352 seconds
[2025-09-02T21:57:27.143+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=3961) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:57:27.144+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:57:27.148+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:57:27.147+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:57:28.397+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:57:28.422+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:57:28.470+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:57:28.469+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:57:28.507+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:57:28.507+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:57:28.537+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.405 seconds
[2025-09-02T21:57:58.707+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=4007) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:57:58.709+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:57:58.713+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:57:58.712+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:58:00.126+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:58:00.164+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:58:00.263+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:58:00.262+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:58:00.334+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:58:00.334+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:58:00.383+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.686 seconds
[2025-09-02T21:58:30.601+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=4053) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:58:30.604+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:58:30.608+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:58:30.608+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:58:31.724+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:58:31.746+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:58:31.790+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:58:31.790+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:58:31.833+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:58:31.833+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:58:31.864+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.283 seconds
[2025-09-02T21:59:02.203+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=4099) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:59:02.206+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:59:02.211+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:59:02.210+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:59:03.452+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:59:03.477+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:59:03.519+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:59:03.519+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:59:03.560+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:59:03.560+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:59:03.593+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.403 seconds
[2025-09-02T21:59:34.138+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=4145) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:59:34.140+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T21:59:34.145+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:59:34.144+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:59:35.308+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T21:59:35.339+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T21:59:35.444+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:59:35.443+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T21:59:35.584+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T21:59:35.583+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T21:59:35.631+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.504 seconds
[2025-09-02T22:00:06.204+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=4191) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:00:06.206+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:00:06.209+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:00:06.208+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:00:07.498+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:00:07.520+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:00:07.573+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:00:07.573+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:00:07.617+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:00:07.617+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:00:07.651+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.459 seconds
[2025-09-02T22:00:38.413+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=4237) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:00:38.418+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:00:38.421+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:00:38.421+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:00:39.783+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:00:39.811+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:00:39.868+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:00:39.867+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:00:39.908+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:00:39.907+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:00:39.940+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.539 seconds
[2025-09-02T22:01:10.190+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=4283) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:01:10.192+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:01:10.203+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:01:10.202+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:01:11.624+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:01:11.664+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:01:11.730+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:01:11.729+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:01:11.795+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:01:11.795+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:01:11.838+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.670 seconds
[2025-09-02T22:01:42.023+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=4327) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:01:42.026+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:01:42.032+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:01:42.031+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:01:43.386+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:01:43.408+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:01:43.442+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:01:43.441+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:01:43.487+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:01:43.486+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:01:43.517+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.525 seconds
[2025-09-02T22:02:14.289+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=4378) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:02:14.292+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:02:14.300+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:02:14.297+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:02:15.432+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:02:15.457+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:02:15.526+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:02:15.525+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:02:15.607+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:02:15.606+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:02:15.664+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.393 seconds
[2025-09-02T22:02:46.424+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=4424) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:02:46.430+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:02:46.437+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:02:46.436+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:02:47.778+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:02:47.799+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:02:47.884+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:02:47.883+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:02:47.938+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:02:47.937+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:02:47.971+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.582 seconds
[2025-09-02T22:03:18.741+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=4470) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:03:18.744+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:03:18.752+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:03:18.751+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:03:19.966+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:03:19.985+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:03:20.021+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:03:20.021+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:03:20.055+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:03:20.055+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:03:20.087+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.370 seconds
[2025-09-02T22:03:50.728+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=4516) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:03:50.732+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:03:50.740+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:03:50.739+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:03:52.210+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:03:52.229+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:03:52.267+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:03:52.267+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:03:52.305+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:03:52.305+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:03:52.335+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.634 seconds
[2025-09-02T22:04:22.590+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=4562) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:04:22.596+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:04:22.601+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:04:22.600+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:04:24.027+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:04:24.046+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:04:24.087+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:04:24.086+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:04:24.127+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:04:24.127+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:04:24.158+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.586 seconds
[2025-09-02T22:04:54.465+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=4607) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:04:54.482+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:04:54.507+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:04:54.505+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:04:58.311+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:04:58.407+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:04:58.557+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:04:58.556+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:04:58.684+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:04:58.684+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:04:58.788+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 4.405 seconds
[2025-09-02T22:05:29.518+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=4643) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:05:29.520+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:05:29.524+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:05:29.523+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:05:31.162+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:05:31.196+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:05:31.287+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:05:31.287+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:05:31.401+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:05:31.401+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:05:31.484+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.979 seconds
[2025-09-02T22:06:02.257+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=4689) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:06:02.259+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:06:02.264+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:06:02.263+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:06:04.079+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:06:04.133+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:06:04.209+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:06:04.208+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:06:04.267+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:06:04.267+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:06:04.306+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 2.072 seconds
[2025-09-02T22:06:35.095+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=4735) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:06:35.101+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:06:35.105+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:06:35.104+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:06:36.814+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:06:36.845+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:06:36.904+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:06:36.904+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:06:36.963+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:06:36.963+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:06:36.998+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.917 seconds
[2025-09-02T22:07:07.813+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=4781) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:07:07.816+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:07:07.821+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:07:07.820+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:07:09.700+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:07:09.732+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:07:09.788+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:07:09.787+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:07:09.838+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:07:09.837+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:07:09.889+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 2.095 seconds
[2025-09-02T22:07:40.803+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=4827) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:07:40.813+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:07:40.821+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:07:40.820+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:07:43.293+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:07:43.320+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:07:43.405+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:07:43.404+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:07:43.448+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:07:43.447+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:07:43.495+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 2.773 seconds
[2025-09-02T22:08:14.006+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=4883) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:08:14.010+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:08:14.016+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:08:14.015+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:08:16.414+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:08:16.442+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:08:16.497+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:08:16.496+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:08:16.542+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:08:16.541+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:08:16.583+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 2.606 seconds
[2025-09-02T22:08:46.713+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=4919) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:08:46.719+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:08:46.722+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:08:46.722+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:08:48.127+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:08:48.163+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:08:48.226+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:08:48.225+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:08:48.286+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:08:48.285+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:08:48.330+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.630 seconds
[2025-09-02T22:09:19.331+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=4976) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:09:19.342+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:09:19.352+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:09:19.351+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:09:23.703+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:09:23.772+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:09:23.889+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:09:23.889+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:09:24.010+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:09:24.009+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:09:24.082+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 4.793 seconds
[2025-09-02T22:09:55.937+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=5027) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:09:55.940+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:09:55.958+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:09:55.957+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:10:00.705+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:10:00.796+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:10:00.955+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:10:00.954+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:10:01.113+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:10:01.112+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:10:01.226+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 5.365 seconds
[2025-09-02T22:10:31.621+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=5085) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:10:31.623+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:10:31.628+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:10:31.627+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:10:33.134+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:10:33.167+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:10:33.228+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:10:33.228+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:10:33.309+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:10:33.306+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:10:33.357+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.749 seconds
[2025-09-02T22:11:03.619+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=5136) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:11:03.620+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:11:03.624+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:11:03.623+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:11:05.110+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:11:05.130+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:11:05.170+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:11:05.170+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:11:05.207+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:11:05.206+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:11:05.242+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.635 seconds
[2025-09-02T22:11:35.583+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=5182) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:11:35.585+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:11:35.588+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:11:35.588+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:11:36.868+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:11:36.890+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:11:36.937+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:11:36.937+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:11:37.003+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:11:37.003+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:11:37.069+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.506 seconds
[2025-09-02T22:12:07.509+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=5228) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:12:07.512+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:12:07.517+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:12:07.515+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:12:08.728+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:12:08.762+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:12:08.814+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:12:08.814+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:12:08.860+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:12:08.860+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:12:08.894+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.401 seconds
[2025-09-02T22:12:39.706+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=5274) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:12:39.709+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:12:39.718+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:12:39.717+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:12:42.767+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:12:42.803+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:12:42.881+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:12:42.881+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:12:42.944+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:12:42.944+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:12:43.008+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 3.326 seconds
[2025-09-02T22:13:13.244+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=5320) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:13:13.248+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:13:13.254+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:13:13.253+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:13:14.687+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:13:14.723+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:13:14.779+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:13:14.778+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:13:14.836+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:13:14.835+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:13:14.874+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.653 seconds
[2025-09-02T22:13:45.149+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=5364) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:13:45.152+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:13:45.165+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:13:45.163+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:13:48.443+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:13:48.464+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:13:48.512+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:13:48.511+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:13:48.569+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:13:48.569+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:13:48.600+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 3.497 seconds
[2025-09-02T22:14:18.813+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=5412) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:14:18.816+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:14:18.821+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:14:18.820+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:14:21.063+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:14:21.153+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:14:21.327+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:14:21.326+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:14:21.417+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:14:21.416+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:14:21.487+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 2.702 seconds
[2025-09-02T22:14:51.680+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=5458) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:14:51.682+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:14:51.686+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:14:51.685+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:14:53.336+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:14:53.449+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:14:53.525+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:14:53.524+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:14:53.581+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:14:53.581+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:14:53.629+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.965 seconds
[2025-09-02T22:15:24.067+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=5504) to work on /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:15:24.069+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_producer_dag.py for tasks to queue
[2025-09-02T22:15:24.073+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:15:24.072+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:15:25.178+0800] INFO - logging_mixin.py:190 - _propagate_log() - CPU 核心數: 8
[2025-09-02T22:15:25.205+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_producer' retrieved from /opt/airflow/dags/hahow_crawler_producer_dag.py
[2025-09-02T22:15:25.242+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:15:25.242+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-02T22:15:25.275+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-02T22:15:25.275+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_producer to 2025-09-01 18:00:00+00:00, run_after=2025-09-02 18:00:00+00:00
[2025-09-02T22:15:25.300+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_producer_dag.py took 1.245 seconds
